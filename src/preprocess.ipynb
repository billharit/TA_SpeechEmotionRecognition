{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from audiomentations import AddGaussianNoise, TimeStretch, PitchShift\n",
    "import shutil\n",
    "import os\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mfcc_list = [26, 40]\n",
    "n_fft_list = [800, 1024, 2048]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "dataset_path = 'SAVEEE'\n",
    "data_list = os.listdir(dataset_path)\n",
    "train_files = []\n",
    "test_files = []\n",
    "\n",
    "labels = {'d': 0, 'h': 1, 's': 2,\n",
    "          'n': 3, 'f': 4, 'a': 5}\n",
    "\n",
    "train_data_sentiment_path = []\n",
    "train_data_sentiment_value = []\n",
    "train_data_sentiment_encoded_value = []\n",
    "\n",
    "test_data_sentiment_path = []\n",
    "test_data_sentiment_value = []\n",
    "test_data_sentiment_encoded_value = []\n",
    "\n",
    "with open('random_state_savee.txt', 'r') as file:\n",
    "    random_state = eval(file.read())\n",
    "\n",
    "random.setstate(random_state)\n",
    "\n",
    "# import random\n",
    "random.seed(1234)\n",
    "for file_path in data_list:\n",
    "    if random.random() < 0.8:\n",
    "        train_files.append(file_path)\n",
    "    else:\n",
    "        test_files.append(file_path)\n",
    "\n",
    "for file in train_files:\n",
    "    file_path = os.path.join(dataset_path, file)\n",
    "    # print(file_path)\n",
    "    train_data_sentiment_path.append(file_path)\n",
    "    train_data_sentiment_value.append(file[3])\n",
    "    train_data_sentiment_encoded_value.append(labels[file[3]])\n",
    "\n",
    "for file in test_files:\n",
    "    file_path = os.path.join(dataset_path, file)\n",
    "    # print(file_path)\n",
    "    test_data_sentiment_path.append(file_path)\n",
    "    test_data_sentiment_value.append(file[3])\n",
    "    test_data_sentiment_encoded_value.append(labels[file[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_into_saved_dataset_SAVEE(num_mfcc,n_fft,hop_length,SAMPLE_RATE = 22050):\n",
    "    # num_mfcc = 40\n",
    "    # # SAMPLE_RATE = librosa.get_samplerate(train_data_sentiment_path[0])\n",
    "    # SAMPLE_RATE = 22050\n",
    "    # n_fft = 2048\n",
    "    # hop_length = 512\n",
    "\n",
    "    train_data = {\n",
    "        \"labels\": [],\n",
    "        \"mfcc\": []\n",
    "    }\n",
    "\n",
    "    test_data = {\n",
    "        \"labels\": [],\n",
    "        \"mfcc\": []\n",
    "    }\n",
    "\n",
    "    for path, value in zip(train_data_sentiment_path, train_data_sentiment_encoded_value):\n",
    "        # print(path)\n",
    "        signal, sample_rate = librosa.load(path, sr=SAMPLE_RATE)\n",
    "        mfcc = librosa.feature.mfcc(\n",
    "            y=signal, sr=SAMPLE_RATE, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "        mfcc = mfcc.T\n",
    "        \n",
    "        train_data['labels'].append(value)\n",
    "        train_data[\"mfcc\"].append(np.asarray(mfcc))\n",
    "\n",
    "    for path, value in zip(test_data_sentiment_path, test_data_sentiment_encoded_value):\n",
    "        # print(path)\n",
    "        signal, sample_rate = librosa.load(path, sr=SAMPLE_RATE)\n",
    "        mfcc = librosa.feature.mfcc(\n",
    "            y=signal, sr=SAMPLE_RATE, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "        mfcc = mfcc.T\n",
    "        \n",
    "        test_data['labels'].append(value)\n",
    "        test_data[\"mfcc\"].append(np.asarray(mfcc))\n",
    "\n",
    "    processed_data_value = np.asarray(train_data['mfcc'])\n",
    "    processed_data_target = np.asarray(train_data[\"labels\"])\n",
    "    processed_test_value = np.asarray(test_data['mfcc'])\n",
    "    processed_test_target = np.asarray(test_data[\"labels\"])\n",
    "    maxLength = max(len(x) for x in processed_data_value)\n",
    "\n",
    "    padded_data_value = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        processed_data_value, maxlen=maxLength ,dtype=\"float32\")\n",
    "    padded_test_value = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        processed_test_value, maxlen=maxLength ,dtype=\"float32\")\n",
    "\n",
    "    np.save('preprocess_dataset/SAVEE/{}-{}-{}_data.npy'.format(num_mfcc,n_fft,hop_length), padded_data_value)\n",
    "    np.save('preprocess_dataset/SAVEE/{}-{}-{}_data_target.npy'.format(num_mfcc,n_fft,hop_length), processed_data_target)\n",
    "    np.save('preprocess_dataset/SAVEE/{}-{}-{}_test.npy'.format(num_mfcc,n_fft,hop_length), padded_test_value)\n",
    "    np.save('preprocess_dataset/SAVEE/{}-{}-{}_test_target.npy'.format(num_mfcc,n_fft,hop_length), processed_test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_9516\\1725014647.py:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  processed_data_value = np.asarray(train_data['mfcc'])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_9516\\1725014647.py:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  processed_test_value = np.asarray(test_data['mfcc'])\n"
     ]
    }
   ],
   "source": [
    "for x in num_mfcc_list:\n",
    "    for y in n_fft_list:\n",
    "        firsthop = int(y/4)\n",
    "        secondhop = int(y/2)\n",
    "        turn_into_saved_dataset_SAVEE(num_mfcc=x, n_fft=y, hop_length=firsthop)\n",
    "        turn_into_saved_dataset_SAVEE(num_mfcc=x, n_fft=y, hop_length=secondhop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAVDESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "train_files = []\n",
    "test_files = []\n",
    "\n",
    "folder_file = os.listdir(\"ravdess_dataset/audio_speech_actors_01-24\")\n",
    "for x in folder_file:\n",
    "    item_file = os.listdir(\"ravdess_dataset/audio_speech_actors_01-24/{}\".format(x))\n",
    "    for y in item_file:\n",
    "        data_list.append(\"ravdess_dataset/audio_speech_actors_01-24/{}/{}\".format(x,y))\n",
    "        \n",
    "train_data_sentiment_path = []\n",
    "train_data_sentiment_value = []\n",
    "train_data_sentiment_encoded_value = []\n",
    "\n",
    "test_data_sentiment_path = []\n",
    "test_data_sentiment_value = []\n",
    "test_data_sentiment_encoded_value = []\n",
    "\n",
    "with open('random_state_savee.txt', 'r') as file:\n",
    "    random_state = eval(file.read())\n",
    "\n",
    "random.setstate(random_state)\n",
    "random.seed(1234)\n",
    "for file_path in data_list:\n",
    "    if random.random() < 0.8:\n",
    "        train_files.append(file_path)\n",
    "    else:\n",
    "        test_files.append(file_path)\n",
    "\n",
    "for file in train_files:\n",
    "    sentiment_code = file[-18:-16]\n",
    "    if sentiment_code == '05':\n",
    "        train_data_sentiment_path.append(file)\n",
    "        train_data_sentiment_value.append('angry')  \n",
    "        train_data_sentiment_encoded_value.append(5)        \n",
    "    elif sentiment_code == '07':\n",
    "        train_data_sentiment_path.append(file)\n",
    "        train_data_sentiment_value.append('disgust')  \n",
    "        train_data_sentiment_encoded_value.append(0)\n",
    "    elif sentiment_code == '06':\n",
    "        train_data_sentiment_path.append(file)\n",
    "        train_data_sentiment_value.append('fear')  \n",
    "        train_data_sentiment_encoded_value.append(4)\n",
    "    elif sentiment_code == '03':\n",
    "        train_data_sentiment_path.append(file)\n",
    "        train_data_sentiment_value.append('happy')  \n",
    "        train_data_sentiment_encoded_value.append(1)\n",
    "    elif sentiment_code == '01':\n",
    "        train_data_sentiment_path.append(file)\n",
    "        train_data_sentiment_value.append('neutral')  \n",
    "        train_data_sentiment_encoded_value.append(3)\n",
    "    elif sentiment_code == '04':\n",
    "        train_data_sentiment_path.append(file)\n",
    "        train_data_sentiment_value.append('sad')  \n",
    "        train_data_sentiment_encoded_value.append(2)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "for file in test_files:\n",
    "    sentiment_code = file[-18:-16]\n",
    "    if sentiment_code == '05':\n",
    "        test_data_sentiment_path.append(file)\n",
    "        test_data_sentiment_value.append('angry')  \n",
    "        test_data_sentiment_encoded_value.append(5)        \n",
    "    elif sentiment_code == '07':\n",
    "        test_data_sentiment_path.append(file)\n",
    "        test_data_sentiment_value.append('disgust')  \n",
    "        test_data_sentiment_encoded_value.append(0)\n",
    "    elif sentiment_code == '06':\n",
    "        test_data_sentiment_path.append(file)\n",
    "        test_data_sentiment_value.append('fear')  \n",
    "        test_data_sentiment_encoded_value.append(4)\n",
    "    elif sentiment_code == '03':\n",
    "        test_data_sentiment_path.append(file)\n",
    "        test_data_sentiment_value.append('happy')  \n",
    "        test_data_sentiment_encoded_value.append(1)\n",
    "    elif sentiment_code == '01':\n",
    "        test_data_sentiment_path.append(file)\n",
    "        test_data_sentiment_value.append('neutral')  \n",
    "        test_data_sentiment_encoded_value.append(3)\n",
    "    elif sentiment_code == '04':\n",
    "        test_data_sentiment_path.append(file)\n",
    "        test_data_sentiment_value.append('sad')  \n",
    "        test_data_sentiment_encoded_value.append(2)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_into_saved_dataset_RAVDESS(num_mfcc,n_fft,hop_length,SAMPLE_RATE = 48000):\n",
    "\n",
    "    train_data = {\n",
    "        \"labels\": [],\n",
    "        \"mfcc\": []\n",
    "    }\n",
    "\n",
    "    test_data = {\n",
    "        \"labels\": [],\n",
    "        \"mfcc\": []\n",
    "    }\n",
    "\n",
    "    for path, value in zip(train_data_sentiment_path, train_data_sentiment_encoded_value):\n",
    "        signal, sample_rate = librosa.load(path)\n",
    "        mfcc = librosa.feature.mfcc(\n",
    "            y=signal, sr=SAMPLE_RATE, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "        mfcc = mfcc.T\n",
    "        \n",
    "        train_data['labels'].append(value)\n",
    "        train_data[\"mfcc\"].append(np.asarray(mfcc))\n",
    "\n",
    "    for path, value in zip(test_data_sentiment_path, test_data_sentiment_encoded_value):\n",
    "        signal, sample_rate = librosa.load(path)\n",
    "        mfcc = librosa.feature.mfcc(\n",
    "            y=signal, sr=SAMPLE_RATE, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "        mfcc = mfcc.T\n",
    "        \n",
    "        test_data['labels'].append(value)\n",
    "        test_data[\"mfcc\"].append(np.asarray(mfcc))\n",
    "\n",
    "\n",
    "\n",
    "    processed_data_value = np.asarray(train_data['mfcc'])\n",
    "    processed_data_target = np.asarray(train_data[\"labels\"])\n",
    "    processed_test_value = np.asarray(test_data['mfcc'])\n",
    "    processed_test_target = np.asarray(test_data[\"labels\"])\n",
    "\n",
    "    maxLength = max(len(x) for x in processed_data_value)\n",
    "\n",
    "    padded_data_value = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        processed_data_value, maxlen=maxLength ,dtype=\"float32\")\n",
    "    padded_test_value = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        processed_test_value, maxlen=maxLength ,dtype=\"float32\")\n",
    "    \n",
    "\n",
    "    np.save('preprocess_dataset/RAVDESS/{}-{}-{}_data.npy'.format(num_mfcc,n_fft,hop_length), padded_data_value)\n",
    "    np.save('preprocess_dataset/RAVDESS/{}-{}-{}_data_target.npy'.format(num_mfcc,n_fft,hop_length), processed_data_target)\n",
    "    np.save('preprocess_dataset/RAVDESS/{}-{}-{}_test.npy'.format(num_mfcc,n_fft,hop_length), padded_test_value)\n",
    "    np.save('preprocess_dataset/RAVDESS/{}-{}-{}_test_target.npy'.format(num_mfcc,n_fft,hop_length), processed_test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_9516\\359682469.py:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  processed_data_value = np.asarray(train_data['mfcc'])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_9516\\359682469.py:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  processed_test_value = np.asarray(test_data['mfcc'])\n"
     ]
    }
   ],
   "source": [
    "for x in num_mfcc_list:\n",
    "    for y in n_fft_list:\n",
    "        firsthop = int(y/4)\n",
    "        secondhop = int(y/2)\n",
    "        turn_into_saved_dataset_RAVDESS(num_mfcc=x, n_fft=y, hop_length=firsthop)\n",
    "        turn_into_saved_dataset_RAVDESS(num_mfcc=x, n_fft=y, hop_length=secondhop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREMA-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "def load_to_dataframe(train_folder_path, test_folder_path):\n",
    "    \"\"\"\n",
    "    Loads train and test data from the specified file folders, and returns them as pandas DataFrame objects.\n",
    "    :param train_folder_path: The path to the folder containing the train data files.\n",
    "    :type train_folder_path: str\n",
    "    :param test_folder_path: The path to the folder containing the test data files.\n",
    "    :type test_folder_path: str\n",
    "    :return: A two pandas DataFrame objects, containing the train and test data, respectively.\n",
    "    :rtype: pandas.core.frame.DataFrame, pandas.core.frame.DataFrame\n",
    "    \"\"\"\n",
    "    train_path = 'dataset/train'\n",
    "    test_path = 'dataset/test'\n",
    "    train_dir_list = os.listdir(train_path)\n",
    "    test_dir_list = os.listdir(test_path)\n",
    "\n",
    "    train_sentiment_value = []\n",
    "    test_sentiment_value = []\n",
    "    train_file_path = []\n",
    "    test_file_path = []\n",
    "\n",
    "    for file in train_dir_list:\n",
    "        train_file_path.append(train_path + '/' + file)\n",
    "        sentiment_code = file.split('_')\n",
    "        if sentiment_code[2] == 'ANG':\n",
    "            train_sentiment_value.append('angry')\n",
    "        elif sentiment_code[2] == 'DIS':\n",
    "            train_sentiment_value.append('disgust')\n",
    "        elif sentiment_code[2] == 'FEA':\n",
    "            train_sentiment_value.append('fear')\n",
    "        elif sentiment_code[2] == 'HAP':\n",
    "            train_sentiment_value.append('happy')\n",
    "        elif sentiment_code[2] == 'NEU':\n",
    "            train_sentiment_value.append('neutral')\n",
    "        elif sentiment_code[2] == 'SAD':\n",
    "            train_sentiment_value.append('sad')\n",
    "        else:\n",
    "            train_sentiment_value.append('unknown')\n",
    "\n",
    "    for file in test_dir_list:\n",
    "        test_file_path.append(test_path + '/' + file)\n",
    "        sentiment_code = file.split('_')\n",
    "        if sentiment_code[2] == 'ANG':\n",
    "            test_sentiment_value.append('angry')\n",
    "        elif sentiment_code[2] == 'DIS':\n",
    "            test_sentiment_value.append('disgust')\n",
    "        elif sentiment_code[2] == 'FEA':\n",
    "            test_sentiment_value.append('fear')\n",
    "        elif sentiment_code[2] == 'HAP':\n",
    "            test_sentiment_value.append('happy')\n",
    "        elif sentiment_code[2] == 'NEU':\n",
    "            test_sentiment_value.append('neutral')\n",
    "        elif sentiment_code[2] == 'SAD':\n",
    "            test_sentiment_value.append('sad')\n",
    "        else:\n",
    "            test_sentiment_value.append('unknown')\n",
    "\n",
    "    train_sentiment_df = pd.DataFrame(\n",
    "        {\"File_Path\": train_file_path, \"Target\": train_sentiment_value})\n",
    "\n",
    "    test_sentiment_df = pd.DataFrame(\n",
    "        {\"File_Path\": test_file_path, \"Target\": test_sentiment_value})\n",
    "\n",
    "    return train_sentiment_df, test_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_into_data_for_model(train_df, test_df, num_mfcc, n_fft, hop_length, SAMPLE_RATE = 16000):\n",
    "    # Set Variable for MFCC\n",
    "\n",
    "    train_data = {\n",
    "        \"labels\": [],\n",
    "        \"mfcc\": []\n",
    "    }\n",
    "\n",
    "    test_data = {\n",
    "        \"labels\": [],\n",
    "        \"mfcc\": []\n",
    "    }\n",
    "\n",
    "    # Encode Categories\n",
    "    labels = {'disgust': 0, 'happy': 1, 'sad': 2,\n",
    "              'neutral': 3, 'fear': 4, 'angry': 5}\n",
    "    train_df_encoded = train_df.replace({'Target': labels}, inplace=False)\n",
    "    test_df_encoded = test_df.replace({'Target': labels}, inplace=False)\n",
    "\n",
    "    for item, row in train_df.iterrows():\n",
    "        train_data['labels'].append(train_df_encoded.iloc[item, 1])\n",
    "        signal, sample_rate = librosa.load(\n",
    "            train_df_encoded.iloc[item, 0], sr=SAMPLE_RATE)\n",
    "        mfcc = librosa.feature.mfcc(\n",
    "            y=signal, sr=SAMPLE_RATE, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "        mfcc = mfcc.T\n",
    "        train_data[\"mfcc\"].append(np.asarray(mfcc))\n",
    "\n",
    "        if item % 300 == 0:\n",
    "            print(\"Train Size:\" + str(math.floor(item)))\n",
    "\n",
    "    for item, row in test_df.iterrows():\n",
    "        test_data['labels'].append(test_df_encoded.iloc[item, 1])\n",
    "        signal, sample_rate = librosa.load(\n",
    "            test_df_encoded.iloc[item, 0], sr=SAMPLE_RATE)\n",
    "        mfcc = librosa.feature.mfcc(\n",
    "            y=signal, sr=SAMPLE_RATE, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "        mfcc = mfcc.T\n",
    "        test_data[\"mfcc\"].append(np.asarray(mfcc))\n",
    "        if item % 300 == 0:\n",
    "            print(\"Test Size:\" + str(math.floor(item)))\n",
    "\n",
    "\n",
    "    processed_data_value = np.asarray(train_data['mfcc'])\n",
    "    processed_data_target = np.asarray(train_data[\"labels\"])\n",
    "    processed_test_value = np.asarray(test_data['mfcc'])\n",
    "    processed_test_target = np.asarray(test_data[\"labels\"])\n",
    "\n",
    "    maxLength = max(len(x) for x in processed_data_value)\n",
    "\n",
    "    padded_data_value = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        processed_data_value, maxlen=maxLength ,dtype=\"float32\")\n",
    "    padded_test_value = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        processed_test_value, maxlen=maxLength ,dtype=\"float32\")\n",
    "\n",
    "    # print(test_data_value.shape)\n",
    "    np.save('preprocess_dataset/CREMA-D/{}-{}-{}_data.npy'.format(num_mfcc,n_fft,hop_length), padded_data_value)\n",
    "    np.save('preprocess_dataset/CREMA-D/{}-{}-{}_data_target.npy'.format(num_mfcc,n_fft,hop_length), processed_data_target)\n",
    "    np.save('preprocess_dataset/CREMA-D/{}-{}-{}_test.npy'.format(num_mfcc,n_fft,hop_length), padded_test_value)\n",
    "    np.save('preprocess_dataset/CREMA-D/{}-{}-{}_test_target.npy'.format(num_mfcc,n_fft,hop_length), processed_test_target)\n",
    "    # return train_data_value, train_data_target, test_data_value, test_data_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = load_to_dataframe('dataset/train', 'dataset/test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size:0\n",
      "Train Size:300\n",
      "Train Size:600\n",
      "Train Size:900\n",
      "Train Size:1200\n",
      "Train Size:1500\n",
      "Train Size:1800\n",
      "Train Size:2100\n",
      "Train Size:2400\n",
      "Train Size:2700\n",
      "Train Size:3000\n",
      "Train Size:3300\n",
      "Train Size:3600\n",
      "Train Size:3900\n",
      "Train Size:4200\n",
      "Train Size:4500\n",
      "Train Size:4800\n",
      "Train Size:5100\n",
      "Train Size:5400\n",
      "Train Size:5700\n",
      "Test Size:0\n",
      "Test Size:300\n",
      "Test Size:600\n",
      "Test Size:900\n",
      "Test Size:1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_9516\\3960131172.py:44: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  processed_data_value = np.asarray(train_data['mfcc'])\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_9516\\3960131172.py:46: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  processed_test_value = np.asarray(test_data['mfcc'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size:0\n",
      "Train Size:300\n",
      "Train Size:600\n",
      "Train Size:900\n",
      "Train Size:1200\n",
      "Train Size:1500\n",
      "Train Size:1800\n",
      "Train Size:2100\n",
      "Train Size:2400\n",
      "Train Size:2700\n",
      "Train Size:3000\n",
      "Train Size:3300\n",
      "Train Size:3600\n",
      "Train Size:3900\n",
      "Train Size:4200\n",
      "Train Size:4500\n",
      "Train Size:4800\n",
      "Train Size:5100\n",
      "Train Size:5400\n",
      "Train Size:5700\n",
      "Test Size:0\n",
      "Test Size:300\n",
      "Test Size:600\n",
      "Test Size:900\n",
      "Test Size:1200\n",
      "Train Size:0\n",
      "Train Size:300\n",
      "Train Size:600\n",
      "Train Size:900\n",
      "Train Size:1200\n",
      "Train Size:1500\n",
      "Train Size:1800\n",
      "Train Size:2100\n",
      "Train Size:2400\n",
      "Train Size:2700\n",
      "Train Size:3000\n",
      "Train Size:3300\n",
      "Train Size:3600\n",
      "Train Size:3900\n",
      "Train Size:4200\n",
      "Train Size:4500\n",
      "Train Size:4800\n",
      "Train Size:5100\n",
      "Train Size:5400\n",
      "Train Size:5700\n",
      "Test Size:0\n",
      "Test Size:300\n",
      "Test Size:600\n",
      "Test Size:900\n",
      "Test Size:1200\n",
      "Train Size:0\n",
      "Train Size:300\n",
      "Train Size:600\n",
      "Train Size:900\n",
      "Train Size:1200\n",
      "Train Size:1500\n",
      "Train Size:1800\n",
      "Train Size:2100\n",
      "Train Size:2400\n",
      "Train Size:2700\n",
      "Train Size:3000\n",
      "Train Size:3300\n",
      "Train Size:3600\n",
      "Train Size:3900\n",
      "Train Size:4200\n",
      "Train Size:4500\n",
      "Train Size:4800\n",
      "Train Size:5100\n",
      "Train Size:5400\n",
      "Train Size:5700\n",
      "Test Size:0\n",
      "Test Size:300\n",
      "Test Size:600\n",
      "Test Size:900\n",
      "Test Size:1200\n",
      "Train Size:0\n",
      "Train Size:300\n",
      "Train Size:600\n",
      "Train Size:900\n",
      "Train Size:1200\n",
      "Train Size:1500\n",
      "Train Size:1800\n",
      "Train Size:2100\n",
      "Train Size:2400\n",
      "Train Size:2700\n",
      "Train Size:3000\n",
      "Train Size:3300\n",
      "Train Size:3600\n",
      "Train Size:3900\n",
      "Train Size:4200\n",
      "Train Size:4500\n",
      "Train Size:4800\n",
      "Train Size:5100\n",
      "Train Size:5400\n",
      "Train Size:5700\n",
      "Test Size:0\n",
      "Test Size:300\n",
      "Test Size:600\n",
      "Test Size:900\n",
      "Test Size:1200\n",
      "Train Size:0\n",
      "Train Size:300\n",
      "Train Size:600\n",
      "Train Size:900\n",
      "Train Size:1200\n",
      "Train Size:1500\n",
      "Train Size:1800\n",
      "Train Size:2100\n",
      "Train Size:2400\n",
      "Train Size:2700\n",
      "Train Size:3000\n",
      "Train Size:3300\n",
      "Train Size:3600\n",
      "Train Size:3900\n",
      "Train Size:4200\n",
      "Train Size:4500\n",
      "Train Size:4800\n",
      "Train Size:5100\n",
      "Train Size:5400\n",
      "Train Size:5700\n",
      "Test Size:0\n",
      "Test Size:300\n",
      "Test Size:600\n",
      "Test Size:900\n",
      "Test Size:1200\n",
      "Train Size:0\n",
      "Train Size:300\n",
      "Train Size:600\n",
      "Train Size:900\n",
      "Train Size:1200\n",
      "Train Size:1500\n",
      "Train Size:1800\n",
      "Train Size:2100\n",
      "Train Size:2400\n",
      "Train Size:2700\n",
      "Train Size:3000\n",
      "Train Size:3300\n",
      "Train Size:3600\n",
      "Train Size:3900\n",
      "Train Size:4200\n",
      "Train Size:4500\n",
      "Train Size:4800\n",
      "Train Size:5100\n",
      "Train Size:5400\n",
      "Train Size:5700\n",
      "Test Size:0\n",
      "Test Size:300\n",
      "Test Size:600\n",
      "Test Size:900\n",
      "Test Size:1200\n",
      "Train Size:0\n",
      "Train Size:300\n",
      "Train Size:600\n",
      "Train Size:900\n",
      "Train Size:1200\n",
      "Train Size:1500\n",
      "Train Size:1800\n",
      "Train Size:2100\n",
      "Train Size:2400\n",
      "Train Size:2700\n",
      "Train Size:3000\n",
      "Train Size:3300\n",
      "Train Size:3600\n",
      "Train Size:3900\n",
      "Train Size:4200\n",
      "Train Size:4500\n",
      "Train Size:4800\n",
      "Train Size:5100\n",
      "Train Size:5400\n",
      "Train Size:5700\n",
      "Test Size:0\n",
      "Test Size:300\n",
      "Test Size:600\n",
      "Test Size:900\n",
      "Test Size:1200\n",
      "Train Size:0\n",
      "Train Size:300\n",
      "Train Size:600\n",
      "Train Size:900\n",
      "Train Size:1200\n",
      "Train Size:1500\n",
      "Train Size:1800\n",
      "Train Size:2100\n",
      "Train Size:2400\n",
      "Train Size:2700\n",
      "Train Size:3000\n",
      "Train Size:3300\n",
      "Train Size:3600\n",
      "Train Size:3900\n",
      "Train Size:4200\n",
      "Train Size:4500\n",
      "Train Size:4800\n",
      "Train Size:5100\n",
      "Train Size:5400\n",
      "Train Size:5700\n",
      "Test Size:0\n",
      "Test Size:300\n",
      "Test Size:600\n",
      "Test Size:900\n",
      "Test Size:1200\n",
      "Train Size:0\n",
      "Train Size:300\n",
      "Train Size:600\n",
      "Train Size:900\n",
      "Train Size:1200\n",
      "Train Size:1500\n",
      "Train Size:1800\n",
      "Train Size:2100\n",
      "Train Size:2400\n",
      "Train Size:2700\n",
      "Train Size:3000\n",
      "Train Size:3300\n",
      "Train Size:3600\n",
      "Train Size:3900\n",
      "Train Size:4200\n",
      "Train Size:4500\n",
      "Train Size:4800\n",
      "Train Size:5100\n",
      "Train Size:5400\n",
      "Train Size:5700\n",
      "Test Size:0\n",
      "Test Size:300\n",
      "Test Size:600\n",
      "Test Size:900\n",
      "Test Size:1200\n",
      "Train Size:0\n",
      "Train Size:300\n",
      "Train Size:600\n",
      "Train Size:900\n",
      "Train Size:1200\n",
      "Train Size:1500\n",
      "Train Size:1800\n",
      "Train Size:2100\n",
      "Train Size:2400\n",
      "Train Size:2700\n",
      "Train Size:3000\n",
      "Train Size:3300\n",
      "Train Size:3600\n",
      "Train Size:3900\n",
      "Train Size:4200\n",
      "Train Size:4500\n",
      "Train Size:4800\n",
      "Train Size:5100\n",
      "Train Size:5400\n",
      "Train Size:5700\n",
      "Test Size:0\n",
      "Test Size:300\n",
      "Test Size:600\n",
      "Test Size:900\n",
      "Test Size:1200\n",
      "Train Size:0\n",
      "Train Size:300\n",
      "Train Size:600\n",
      "Train Size:900\n",
      "Train Size:1200\n",
      "Train Size:1500\n",
      "Train Size:1800\n",
      "Train Size:2100\n",
      "Train Size:2400\n",
      "Train Size:2700\n",
      "Train Size:3000\n",
      "Train Size:3300\n",
      "Train Size:3600\n",
      "Train Size:3900\n",
      "Train Size:4200\n",
      "Train Size:4500\n",
      "Train Size:4800\n",
      "Train Size:5100\n",
      "Train Size:5400\n",
      "Train Size:5700\n",
      "Test Size:0\n",
      "Test Size:300\n",
      "Test Size:600\n",
      "Test Size:900\n",
      "Test Size:1200\n"
     ]
    }
   ],
   "source": [
    "for x in num_mfcc_list:\n",
    "    for y in n_fft_list:\n",
    "        firsthop = int(y/4)\n",
    "        secondhop = int(y/2)\n",
    "        turn_into_data_for_model(train_df, test_df, num_mfcc=x, n_fft=y, hop_length=firsthop)\n",
    "        turn_into_data_for_model(train_df, test_df, num_mfcc=x, n_fft=y, hop_length=secondhop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328, 308, 26)\n",
      "(847, 228, 26)\n",
      "(5953, 156, 26)\n"
     ]
    }
   ],
   "source": [
    "x = np.load(\"preprocess_dataset/SAVEE/26-2048-512_data.npy\")\n",
    "print(x.shape)\n",
    "y = np.load(\"preprocess_dataset/RAVDESS/26-2048-512_data.npy\")\n",
    "print(y.shape)\n",
    "z = np.load(\"preprocess_dataset/CREMA-D/26-2048-512_data.npy\")\n",
    "print(z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328, 788, 13)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_value = np.load(\n",
    "    \"preprocessed_dataset_lama/CREMA-D/40-1024-512_data.npy\")\n",
    "train_data_target = np.load(\n",
    "    \"preprocessed_dataset_lama/CREMA-D/40-1024-512_data.npy\")\n",
    "test_data_value = np.load(\n",
    "    \"preprocessed_dataset_lama/CREMA-D/40-1024-512_data.npy\")\n",
    "test_data_target = np.load(\n",
    "    \"preprocessed_dataset_lama/CREMA-D/40-1024-512_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5953, 156, 40)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5953, 156, 40)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        ...,\n",
       "        [-5.24512207e+02,  1.12205383e+02,  3.52632217e+01, ...,\n",
       "         -4.86970520e+00, -3.55292130e+00, -1.82775474e+00],\n",
       "        [-5.27892639e+02,  1.08647430e+02,  2.48086662e+01, ...,\n",
       "         -6.29737377e+00, -1.24458599e+00,  8.30532551e-01],\n",
       "        [-5.63413208e+02,  1.07229431e+02,  2.64484749e+01, ...,\n",
       "         -4.20998049e+00,  1.06432319e+00, -3.00609899e+00]],\n",
       "\n",
       "       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        ...,\n",
       "        [-5.23959839e+02,  1.12656296e+02,  2.77732430e+01, ...,\n",
       "         -5.53771925e+00, -3.21555138e-03, -4.43607187e+00],\n",
       "        [-5.31137512e+02,  1.13417480e+02,  4.26006775e+01, ...,\n",
       "         -6.15498352e+00, -6.33268547e+00, -8.30254364e+00],\n",
       "        [-5.29234253e+02,  1.18167809e+02,  3.52623444e+01, ...,\n",
       "         -4.55444384e+00, -1.39848077e+00, -5.19996071e+00]],\n",
       "\n",
       "       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        ...,\n",
       "        [-5.27224670e+02,  1.05472672e+02,  3.24800148e+01, ...,\n",
       "         -9.71544266e+00, -3.32673454e+00,  2.37276363e+00],\n",
       "        [-5.18026489e+02,  1.04773422e+02,  3.18352623e+01, ...,\n",
       "         -6.33654952e-01, -5.91219902e+00, -3.48766518e+00],\n",
       "        [-5.30423462e+02,  1.03903282e+02,  3.07380905e+01, ...,\n",
       "         -8.64295959e+00, -2.30792546e+00,  1.44077152e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        ...,\n",
       "        [-5.78853882e+02,  1.11318024e+02,  5.73806229e+01, ...,\n",
       "         -3.56408358e+00, -1.19746041e+00,  4.67723131e-01],\n",
       "        [-5.78694214e+02,  1.12051666e+02,  5.40935478e+01, ...,\n",
       "         -7.03207016e-01, -3.35973930e+00, -1.87471652e+00],\n",
       "        [-5.51757629e+02,  1.12250885e+02,  2.21485252e+01, ...,\n",
       "         -1.62381935e+00, -2.18927479e+00, -2.00493360e+00]],\n",
       "\n",
       "       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        ...,\n",
       "        [-4.12404541e+02,  1.41524982e+01,  5.88316994e+01, ...,\n",
       "         -6.83247566e-01, -1.76743102e+00, -2.67302608e+00],\n",
       "        [-4.67492432e+02,  4.39217720e+01,  6.57858887e+01, ...,\n",
       "         -5.85676289e+00, -6.76530361e+00, -1.68008232e+00],\n",
       "        [-5.39616760e+02,  9.31727753e+01,  5.68975601e+01, ...,\n",
       "         -5.57823300e-01, -6.81499600e-01,  2.84056950e+00]],\n",
       "\n",
       "       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        ...,\n",
       "        [-5.84341125e+02,  1.13603325e+02,  6.66221542e+01, ...,\n",
       "         -1.22261381e+00,  2.24926531e-01,  1.81099844e+00],\n",
       "        [-5.92722839e+02,  1.16596710e+02,  5.92232056e+01, ...,\n",
       "         -3.86372972e+00, -3.52342510e+00,  2.90746212e+00],\n",
       "        [-5.50607788e+02,  1.18369156e+02,  2.52528000e+01, ...,\n",
       "         -3.19519901e+00,  1.34416068e+00, -3.03132391e+00]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
