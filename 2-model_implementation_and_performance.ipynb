{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook Shows how to build a Model using TensorFlow and how to train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotplib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 Models that is being used in my thesis that can be made using TensorFlow Libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGGNet16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorFlow Sequential\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# You could create VGGNet from Scratch (Read the Original Paper) or just use the built in architecture from TensorFlow which iis the same\n",
    "# weights should be set if want to use transfer learning\n",
    "# include_top should be false as we use different classifier\n",
    "# input shape should be (MFCCs Coefficient, Length of the Frame, 1)\n",
    "base_model = tf.keras.applications.vgg16.VGG16(\n",
    "    include_top=False, weights=None, input_shape=(train_data_value.shape[1], train_data_value.shape[2], 1))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(512, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(512, activation=\"relu\"))\n",
    "# The last layer uses 6 as parameter for 6 classes \n",
    "model.add(tf.keras.layers.Dense(6, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorFlow Sequential\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# You could create ResNet50v2 from Scratch (Read the Original Paper) or just use the built in architecture from TensorFlow which iis the same\n",
    "# weights should be set if want to use transfer learning\n",
    "# include_top should be false as we use different classifier\n",
    "# input shape should be (MFCCs Coefficient, Length of the Frame, 1)\n",
    "base_model = tf.keras.applications.resnet_v2.ResNet50V2(\n",
    "    include_top=False, weights=None, input_shape=(train_data_value.shape[1], train_data_value.shape[2], 1))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "# The last layer uses 6 as parameter for 6 classes \n",
    "model.add(tf.keras.layers.Dense(6, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50v2-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorFlow Sequential\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# You could create ResNet50v2 from Scratch (Read the Original Paper) or just use the built in architecture from TensorFlow which iis the same\n",
    "# weights should be set if want to use transfer learning\n",
    "# include_top should be false as we use different classifier\n",
    "# input shape should be (MFCCs Coefficient, Length of the Frame, 1)\n",
    "base_model = tf.keras.applications.resnet_v2.ResNet50V2(\n",
    "    include_top=False, weights=None, input_shape=(train_data_value.shape[1], train_data_value.shape[2], 1))\n",
    "\n",
    "# Flatten the Matrixes with Time Distributed Properties\n",
    "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten()))\n",
    "\n",
    "# The Bidirectional LSTM Layer\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)))\n",
    "\n",
    "# The last layer uses 6 as parameter for 6 classes \n",
    "model.add(tf.keras.layers.Dense(6, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-LSTM (Main Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TensorFlow Sequential\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# input shape should be (MFCCs Coefficient, Length of the Frame, 1)\n",
    "# The following model uses 4-block CNN which extract features from MFCCs and then fed into Bi-LSTM layer before the final classifier\n",
    "\n",
    "# 1st CNN-Block\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), input_shape=(\n",
    "    train_data_value.shape[1], train_data_value.shape[2], 1)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('elu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(\n",
    "    (2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "# 2nd CNN-Block\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('elu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(\n",
    "    (2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "# 3rd CNN-Block\n",
    "model.add(tf.keras.layers.Conv2D(128, (2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('elu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(\n",
    "    (2, 2), strides=(2, 2), padding='same'))\n",
    "\n",
    "# 4th CNN-Block\n",
    "model.add(tf.keras.layers.Conv2D(128, (2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('elu'))\n",
    "\n",
    "# Flatten the Matrixes with Time Distributed Properties\n",
    "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten()))\n",
    "\n",
    "# The Bidirectional LSTM Layer\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256)))\n",
    "\n",
    "# The last layer uses 6 as parameter for 6 classes \n",
    "model.add(tf.keras.layers.Dense(6, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section show on how to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Optimizer, Learning Rate, and Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning Optimizer to Model\n",
    "optimiser = tf.keras.optimizers.get(\"adam\")\n",
    "\n",
    "# Assigning Learning Rate to Model\n",
    "optimiser.learning_rate.assign(0.0001)\n",
    "\n",
    "# For more complete information about optimizer you could view the TensorFlow Documentation\n",
    "\n",
    "# Early Stop Function that Track Validation Loss\n",
    "# This will stop the training if the validation loss does not improve\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5)\n",
    "\n",
    "# This will be used as training calllback to save the log\n",
    "csv_logger = tf.keras.callbacks.CSVLogger('Model_Log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For more Complete guide see the TensorFlow Documentation\n",
    "# You Could use validation_split=0.8 instead of validation data if you dont split it manually\n",
    "# batch_size refer to data looped/step. set it as 32 for standard value, increase it to increase performance\n",
    "# epochs refers to the number of training loop\n",
    "# callbacks should filled with early stopping and csv logger callback (or your additional callbacks)\n",
    "history = model.fit(train_data_value, train_data_target, validation_data=(\n",
    "    validation_data_value, validation_data_target), batch_size=32, epochs=60, callbacks=[csv_logger, early_stop])\n",
    "\n",
    "# Saving the model\n",
    "model.save(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous trained model from the Model Training Section, we can see the performance model with various metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Dataset\n",
    "test_data_value = np.load(\"test_data_value.npy\")\n",
    "test_data_target = np.load(\"test_data_target.npy\")\n",
    "\n",
    "# if uses saved model\n",
    "# model = tf.keras.models.load_model(\"...\") \n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(test_data_value, test_data_target, verbose=0)\n",
    "\n",
    "# Get the prediction for the test data\n",
    "test_pred = model.predict(test_data_value)\n",
    "\n",
    "# Compute the overall accuracy/prediction\n",
    "test_pred_classes = np.argmax(test_pred, axis=1)\n",
    "test_true_classes = np.argmax(test_data_target, axis=1)\n",
    "test_accuracy = np.mean(test_pred_classes == test_true_classes)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix, F1-Score, Recall, And Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Saved Model\n",
    "loaded_model = tf.keras.models.load_model(\"...\")\n",
    "\n",
    "# Load Test Dataset\n",
    "test_data_value = np.load(\"test_data_value.npy\")\n",
    "test_data_target = np.load(\"test_data_target.npy\")\n",
    "\n",
    "predictions = loaded_model.predict(test_data_value)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "confusion_matrix = tf.math.confusion_matrix(test_data_target, predicted_labels)\n",
    "\n",
    "test_loss, test_acc = loaded_model.evaluate(test_data_value, test_data_target)\n",
    "\n",
    "y_pred = loaded_model.predict(test_data_value)\n",
    "y_pred = tf.argmax(y_pred, axis=1)\n",
    "y_true_tensor = tf.convert_to_tensor(test_data_target)\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_mtx = tf.math.confusion_matrix(y_true_tensor, y_pred)\n",
    "\n",
    "# Normalized Version\n",
    "confusion_array = confusion_mtx.numpy()\n",
    "row_sums = confusion_array.sum(axis=1, keepdims=True)\n",
    "normalized_confusion_array = confusion_array / row_sums\n",
    "\n",
    "\n",
    "# The Following Line is to Plot the Confusion Matrix into Heatmap\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "axs[0].set_title('Confusion Matrix')\n",
    "sns.heatmap(confusion_mtx,\n",
    "            xticklabels=[\"disgust\", \"happy\", \"sad\", \"neutral\", \"fear\", \"angry\"],\n",
    "            yticklabels=[\"disgust\", \"happy\", \"sad\", \"neutral\", \"fear\", \"angry\"],\n",
    "            annot=True, fmt='g', cbar=False, ax=axs[0], annot_kws={\"fontsize\": 12})\n",
    "axs[0].set_xlabel('Prediction', fontsize=12)\n",
    "axs[0].set_ylabel('Label', fontsize=12)\n",
    "axs[0].set_xticklabels(axs[1].get_xticklabels(), fontsize=12)\n",
    "axs[0].set_yticklabels(axs[1].get_yticklabels(), fontsize=12)\n",
    "\n",
    "axs[1].set_title('Normalized Confusion Matrix')\n",
    "sns.heatmap(normalized_confusion_array,\n",
    "            xticklabels=[\"disgust\", \"happy\", \"sad\", \"neutral\", \"fear\", \"angry\"],\n",
    "            yticklabels=[\"disgust\", \"happy\", \"sad\", \"neutral\", \"fear\", \"angry\"],\n",
    "            annot=True, fmt='.2%', cbar=False, ax=axs[1], annot_kws={\"fontsize\": 12})\n",
    "axs[1].set_xlabel('Prediction', fontsize=12)\n",
    "axs[1].set_ylabel('Label', fontsize=12)\n",
    "axs[1].set_xticklabels(axs[1].get_xticklabels(), fontsize=12)\n",
    "axs[1].set_yticklabels(axs[1].get_yticklabels(), fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1-Score, Recall, And Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Saved Model\n",
    "loaded_model = tf.keras.models.load_model(\"...\")\n",
    "\n",
    "# Load Test Dataset\n",
    "test_data_value = np.load(\"test_data_value.npy\")\n",
    "test_data_target = np.load(\"test_data_target.npy\")\n",
    "\n",
    "# Predict\n",
    "predictions = loaded_model.predict(test_data_value)  \n",
    "predicted_labels = tf.argmax(predictions, axis=1)\n",
    "\n",
    "# Get the Precision, Recall, and F1\n",
    "precision = precision_score(test_data_target, predicted_labels, average=None)\n",
    "recall = recall_score(test_data_target, predicted_labels, average=None)\n",
    "f1 = f1_score(test_data_target, predicted_labels, average=None)\n",
    "\n",
    "# Print the Precision, Recall, and F1\n",
    "print(\"Each Class\")\n",
    "print(\"Precision:{}\".format(precision))\n",
    "print(\"Recall:{}\".format(recall))\n",
    "print(\"F1:{}\".format(f1))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
